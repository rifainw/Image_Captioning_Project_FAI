{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pickle import dump, load\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.36.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b1de0c3b264d2d89ada370af18514c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Small library for seeing the progress of loops\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "# Loading a text file into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images with their captions\n",
    "\n",
    "def all_img_captions(filename):\n",
    "    file = load_doc(filename)\n",
    "    captions = file.split('\\n')\n",
    "    descriptions = {}\n",
    "    for caption in captions[:-1]:\n",
    "        img, caption = caption.split('\\t')\n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]] = [ caption ]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning - lower casing, removing puntuations, and words containing numbers\n",
    "\n",
    "def cleaning_text(captions):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    for img,caps in captions.items():\n",
    "        for i,img_caption in enumerate(caps):\n",
    "            img_caption.replace(\"-\",\" \")\n",
    "            desc = img_caption.split()\n",
    "            #converts to lowercase\n",
    "            desc = [word.lower() for word in desc]\n",
    "            #remove punctuation from each token\n",
    "            desc = [word.translate(table) for word in desc]\n",
    "            #remove hanging 's and a \n",
    "            desc = [word for word in desc if(len(word)>1)]\n",
    "            #remove tokens with numbers in them\n",
    "            desc = [word for word in desc if(word.isalpha())]\n",
    "            #convert back to string\n",
    "            img_caption = ' '.join(desc)\n",
    "            captions[img][i]= img_caption\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary of all unique words\n",
    "\n",
    "def text_vocabulary(descriptions):\n",
    "    vocab = set()\n",
    "    for key in descriptions.keys():\n",
    "        [vocab.update(d.split()) for d in descriptions[key]]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All descriptions in one file \n",
    "\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + '\\t' + desc)\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(filename,\"w\")\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path according to project folder\n",
    "\n",
    "dataset_text = r\"C:\\Users\\ASUS\\Anaconda3\\envs\\FAIEnv\\python-project-image-caption-generator\\Flickr8k_text\"\n",
    "dataset_images = r\"C:\\Users\\ASUS\\Anaconda3\\envs\\FAIEnv\\python-project-image-caption-generator\\Flicker8k_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our text data\n",
    "\n",
    "filename = dataset_text + \"/\" + \"Flickr8k.token.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of descriptions = 8092\n"
     ]
    }
   ],
   "source": [
    "descriptions = all_img_captions(filename)\n",
    "print(\"Length of descriptions =\",len(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the descriptions\n",
    "\n",
    "clean_descriptions = cleaning_text(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building vocabulary\n",
    "\n",
    "vocabulary = text_vocabulary(clean_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary = 10908\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of vocabulary =\",len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving each description to file\n",
    "\n",
    "save_descriptions(clean_descriptions,\"my_descriptions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the feature vector from all images\n",
    "\n",
    "def extract_features(directory):\n",
    "        model = Xception( include_top=False, pooling='avg' )\n",
    "        features = {}\n",
    "        for img in tqdm(os.listdir(directory)):\n",
    "            filename = directory + \"/\" + img\n",
    "            image = Image.open(filename)\n",
    "            image = image.resize((299,299))\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            #image = preprocess_input(image)\n",
    "            image = image/127.5\n",
    "            image = image - 1.0\n",
    "            feature = model.predict(image)\n",
    "            features[img] = feature\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e36fb495bbf4887bc9a6ff5387ee2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8091), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#2048 feature vector\n",
    "\n",
    "features = extract_features(dataset_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(features, open(\"features.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load(open(\"features.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset for training the model\n",
    "\n",
    "def load_photos(filename):\n",
    "    file = load_doc(filename)\n",
    "    photos = file.split(\"\\n\")[:-1]\n",
    "    return photos\n",
    "\n",
    "def load_clean_descriptions(filename, photos): \n",
    "    #loading clean_descriptions\n",
    "    file = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    for line in file.split(\"\\n\"):\n",
    "        words = line.split()\n",
    "        if len(words)<1 :\n",
    "            continue\n",
    "        image, image_caption = words[0], words[1:]\n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
    "            descriptions[image].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "def load_features(photos):\n",
    "    #loading all features\n",
    "    all_features = load(open(\"features.p\",\"rb\"))\n",
    "    #selecting only needed features\n",
    "    features = {k:all_features[k] for k in photos}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename2 = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = load_photos(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = load_clean_descriptions(\"my_descriptions.txt\", train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = load_features(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras library for tokenizer function\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dictionary to clean list of descriptions\n",
    "\n",
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tokenizer class \n",
    "# this will vectorise text corpus\n",
    "# each integer will represent token in dictionary\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give each word an index, and store that into my_tokenizer.p pickle file\n",
    "\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "dump(tokenizer, open('my_tokenizer.p', 'wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9501"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate maximum length of descriptions\n",
    "\n",
    "def max_length(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in desc_list)\n",
    "    \n",
    "max_length = max_length(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Generator\n",
    "\n",
    "# create input-output sequence pairs from the image description\n",
    "\n",
    "# data generator, used by model.fit_generator()\n",
    "\n",
    "def data_generator(descriptions, features, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for key, description_list in descriptions.items():\n",
    "            #retrieve photo features\n",
    "            feature = features[key][0]\n",
    "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
    "            yield [[input_image, input_sequence], output_word]\n",
    "            \n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check the shape of the input and output for your model\n",
    "\n",
    "[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51, 2048), (51, 34), (51, 9501))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the CNN-RNN model\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# Define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    \n",
    "    # LSTM sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    \n",
    "    # Merging both models\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='my_model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n",
      "Photos: train= 6000\n",
      "Vocabulary Size: 9501\n",
      "Description Length:  34\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "print('Dataset: ', len(train_imgs))\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "print('Photos: train=', len(train_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydot) (2.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pydot as pyd\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "keras.utils.vis_utils.pydot = pyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 34)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      2432256     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          524544      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 9501)         2441757     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,989,661\n",
      "Trainable params: 5,989,661\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 10\n",
    "steps = len(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 563/6000 [=>............................] - ETA: 1:48:41 - loss: 9.16 - ETA: 1:12:52 - loss: 9.14 - ETA: 1:04:05 - loss: 9.11 - ETA: 57:28 - loss: 9.0832 - ETA: 54:12 - loss: 9.06 - ETA: 51:44 - loss: 9.00 - ETA: 49:35 - loss: 8.94 - ETA: 48:13 - loss: 8.85 - ETA: 47:50 - loss: 8.72 - ETA: 46:35 - loss: 8.60 - ETA: 45:33 - loss: 8.49 - ETA: 45:08 - loss: 8.42 - ETA: 44:26 - loss: 8.54 - ETA: 43:38 - loss: 8.46 - ETA: 43:21 - loss: 8.35 - ETA: 42:54 - loss: 8.23 - ETA: 42:38 - loss: 8.22 - ETA: 42:12 - loss: 8.10 - ETA: 41:46 - loss: 8.02 - ETA: 41:26 - loss: 7.90 - ETA: 41:14 - loss: 7.86 - ETA: 41:08 - loss: 7.76 - ETA: 40:56 - loss: 7.70 - ETA: 40:53 - loss: 7.63 - ETA: 40:52 - loss: 7.57 - ETA: 40:44 - loss: 7.53 - ETA: 40:40 - loss: 7.50 - ETA: 40:28 - loss: 7.47 - ETA: 40:25 - loss: 7.44 - ETA: 40:35 - loss: 7.42 - ETA: 40:23 - loss: 7.35 - ETA: 40:18 - loss: 7.34 - ETA: 40:07 - loss: 7.31 - ETA: 40:02 - loss: 7.29 - ETA: 39:44 - loss: 7.29 - ETA: 39:39 - loss: 7.27 - ETA: 39:49 - loss: 7.25 - ETA: 39:48 - loss: 7.19 - ETA: 39:45 - loss: 7.17 - ETA: 39:47 - loss: 7.15 - ETA: 39:36 - loss: 7.09 - ETA: 39:30 - loss: 7.08 - ETA: 39:30 - loss: 7.05 - ETA: 39:29 - loss: 7.03 - ETA: 39:21 - loss: 6.98 - ETA: 39:19 - loss: 6.95 - ETA: 39:11 - loss: 6.96 - ETA: 39:06 - loss: 6.96 - ETA: 39:06 - loss: 6.94 - ETA: 39:00 - loss: 6.95 - ETA: 38:53 - loss: 6.94 - ETA: 38:51 - loss: 6.93 - ETA: 38:51 - loss: 6.91 - ETA: 38:49 - loss: 6.90 - ETA: 38:47 - loss: 6.87 - ETA: 38:46 - loss: 6.86 - ETA: 38:38 - loss: 6.83 - ETA: 38:33 - loss: 6.81 - ETA: 38:41 - loss: 6.80 - ETA: 38:49 - loss: 6.81 - ETA: 38:55 - loss: 6.80 - ETA: 38:59 - loss: 6.81 - ETA: 38:53 - loss: 6.80 - ETA: 38:55 - loss: 6.79 - ETA: 38:50 - loss: 6.76 - ETA: 38:45 - loss: 6.74 - ETA: 38:39 - loss: 6.74 - ETA: 38:37 - loss: 6.70 - ETA: 38:37 - loss: 6.68 - ETA: 38:37 - loss: 6.67 - ETA: 38:34 - loss: 6.65 - ETA: 38:31 - loss: 6.63 - ETA: 38:25 - loss: 6.63 - ETA: 38:25 - loss: 6.61 - ETA: 38:21 - loss: 6.61 - ETA: 38:15 - loss: 6.61 - ETA: 38:16 - loss: 6.61 - ETA: 38:13 - loss: 6.60 - ETA: 38:08 - loss: 6.60 - ETA: 38:07 - loss: 6.60 - ETA: 38:08 - loss: 6.60 - ETA: 38:10 - loss: 6.62 - ETA: 38:13 - loss: 6.61 - ETA: 38:12 - loss: 6.60 - ETA: 38:10 - loss: 6.61 - ETA: 38:17 - loss: 6.60 - ETA: 38:22 - loss: 6.61 - ETA: 38:25 - loss: 6.63 - ETA: 38:28 - loss: 6.64 - ETA: 38:33 - loss: 6.66 - ETA: 38:41 - loss: 6.67 - ETA: 38:44 - loss: 6.69 - ETA: 38:38 - loss: 6.70 - ETA: 38:36 - loss: 6.69 - ETA: 38:37 - loss: 6.70 - ETA: 38:42 - loss: 6.72 - ETA: 38:49 - loss: 6.71 - ETA: 38:57 - loss: 6.71 - ETA: 39:01 - loss: 6.71 - ETA: 39:07 - loss: 6.71 - ETA: 39:14 - loss: 6.71 - ETA: 39:19 - loss: 6.72 - ETA: 39:21 - loss: 6.73 - ETA: 39:20 - loss: 6.73 - ETA: 39:19 - loss: 6.74 - ETA: 39:18 - loss: 6.75 - ETA: 39:14 - loss: 6.76 - ETA: 39:12 - loss: 6.76 - ETA: 39:11 - loss: 6.75 - ETA: 39:05 - loss: 6.75 - ETA: 39:06 - loss: 6.75 - ETA: 39:05 - loss: 6.76 - ETA: 39:02 - loss: 6.76 - ETA: 38:59 - loss: 6.75 - ETA: 38:57 - loss: 6.75 - ETA: 38:55 - loss: 6.75 - ETA: 38:54 - loss: 6.75 - ETA: 38:55 - loss: 6.75 - ETA: 38:55 - loss: 6.75 - ETA: 38:56 - loss: 6.74 - ETA: 38:53 - loss: 6.74 - ETA: 38:52 - loss: 6.75 - ETA: 38:49 - loss: 6.74 - ETA: 38:47 - loss: 6.74 - ETA: 38:45 - loss: 6.74 - ETA: 38:43 - loss: 6.75 - ETA: 38:40 - loss: 6.74 - ETA: 38:38 - loss: 6.73 - ETA: 38:36 - loss: 6.72 - ETA: 38:36 - loss: 6.72 - ETA: 38:35 - loss: 6.72 - ETA: 38:35 - loss: 6.72 - ETA: 38:34 - loss: 6.71 - ETA: 38:33 - loss: 6.71 - ETA: 38:33 - loss: 6.70 - ETA: 38:32 - loss: 6.69 - ETA: 38:29 - loss: 6.69 - ETA: 38:30 - loss: 6.68 - ETA: 38:28 - loss: 6.68 - ETA: 38:28 - loss: 6.68 - ETA: 38:24 - loss: 6.68 - ETA: 38:22 - loss: 6.68 - ETA: 38:20 - loss: 6.67 - ETA: 38:18 - loss: 6.66 - ETA: 38:15 - loss: 6.65 - ETA: 38:13 - loss: 6.65 - ETA: 38:14 - loss: 6.65 - ETA: 38:11 - loss: 6.64 - ETA: 38:10 - loss: 6.65 - ETA: 38:09 - loss: 6.65 - ETA: 38:07 - loss: 6.65 - ETA: 38:05 - loss: 6.65 - ETA: 38:05 - loss: 6.65 - ETA: 38:03 - loss: 6.64 - ETA: 38:03 - loss: 6.64 - ETA: 38:04 - loss: 6.64 - ETA: 38:03 - loss: 6.64 - ETA: 38:03 - loss: 6.65 - ETA: 38:01 - loss: 6.65 - ETA: 37:59 - loss: 6.65 - ETA: 37:58 - loss: 6.65 - ETA: 37:56 - loss: 6.64 - ETA: 37:55 - loss: 6.64 - ETA: 37:53 - loss: 6.64 - ETA: 37:52 - loss: 6.64 - ETA: 37:51 - loss: 6.64 - ETA: 37:51 - loss: 6.63 - ETA: 37:51 - loss: 6.64 - ETA: 37:49 - loss: 6.63 - ETA: 37:48 - loss: 6.63 - ETA: 37:47 - loss: 6.63 - ETA: 37:46 - loss: 6.62 - ETA: 37:45 - loss: 6.61 - ETA: 37:44 - loss: 6.61 - ETA: 37:43 - loss: 6.60 - ETA: 37:44 - loss: 6.59 - ETA: 37:43 - loss: 6.59 - ETA: 37:41 - loss: 6.59 - ETA: 37:40 - loss: 6.59 - ETA: 37:39 - loss: 6.60 - ETA: 37:37 - loss: 6.59 - ETA: 37:34 - loss: 6.59 - ETA: 37:34 - loss: 6.59 - ETA: 37:35 - loss: 6.58 - ETA: 37:32 - loss: 6.58 - ETA: 37:31 - loss: 6.58 - ETA: 37:29 - loss: 6.58 - ETA: 37:27 - loss: 6.58 - ETA: 37:26 - loss: 6.57 - ETA: 37:25 - loss: 6.57 - ETA: 37:23 - loss: 6.56 - ETA: 37:21 - loss: 6.56 - ETA: 37:21 - loss: 6.55 - ETA: 37:20 - loss: 6.54 - ETA: 37:18 - loss: 6.54 - ETA: 37:17 - loss: 6.53 - ETA: 37:16 - loss: 6.52 - ETA: 37:16 - loss: 6.52 - ETA: 37:16 - loss: 6.52 - ETA: 37:15 - loss: 6.51 - ETA: 37:14 - loss: 6.51 - ETA: 37:12 - loss: 6.51 - ETA: 37:12 - loss: 6.51 - ETA: 37:09 - loss: 6.50 - ETA: 37:07 - loss: 6.50 - ETA: 37:07 - loss: 6.49 - ETA: 37:05 - loss: 6.49 - ETA: 37:04 - loss: 6.48 - ETA: 37:05 - loss: 6.48 - ETA: 37:03 - loss: 6.48 - ETA: 37:03 - loss: 6.47 - ETA: 37:03 - loss: 6.47 - ETA: 37:01 - loss: 6.47 - ETA: 36:59 - loss: 6.47 - ETA: 36:59 - loss: 6.47 - ETA: 36:57 - loss: 6.47 - ETA: 36:56 - loss: 6.46 - ETA: 36:54 - loss: 6.46 - ETA: 36:54 - loss: 6.45 - ETA: 36:52 - loss: 6.44 - ETA: 36:52 - loss: 6.44 - ETA: 36:52 - loss: 6.44 - ETA: 36:51 - loss: 6.43 - ETA: 36:49 - loss: 6.43 - ETA: 36:51 - loss: 6.43 - ETA: 36:51 - loss: 6.42 - ETA: 36:49 - loss: 6.42 - ETA: 36:49 - loss: 6.41 - ETA: 36:48 - loss: 6.41 - ETA: 36:47 - loss: 6.41 - ETA: 36:48 - loss: 6.40 - ETA: 36:48 - loss: 6.40 - ETA: 36:46 - loss: 6.40 - ETA: 36:49 - loss: 6.39 - ETA: 36:47 - loss: 6.39 - ETA: 36:47 - loss: 6.39 - ETA: 36:47 - loss: 6.39 - ETA: 36:45 - loss: 6.38 - ETA: 36:45 - loss: 6.38 - ETA: 36:45 - loss: 6.38 - ETA: 36:44 - loss: 6.39 - ETA: 36:43 - loss: 6.39 - ETA: 36:42 - loss: 6.38 - ETA: 36:41 - loss: 6.39 - ETA: 36:41 - loss: 6.39 - ETA: 36:40 - loss: 6.39 - ETA: 36:39 - loss: 6.40 - ETA: 36:39 - loss: 6.40 - ETA: 36:39 - loss: 6.40 - ETA: 36:37 - loss: 6.40 - ETA: 36:36 - loss: 6.40 - ETA: 36:35 - loss: 6.40 - ETA: 36:35 - loss: 6.40 - ETA: 36:33 - loss: 6.40 - ETA: 36:33 - loss: 6.39 - ETA: 36:32 - loss: 6.40 - ETA: 36:32 - loss: 6.40 - ETA: 36:31 - loss: 6.40 - ETA: 36:30 - loss: 6.40 - ETA: 36:28 - loss: 6.41 - ETA: 36:29 - loss: 6.40 - ETA: 36:28 - loss: 6.41 - ETA: 36:27 - loss: 6.41 - ETA: 36:25 - loss: 6.41 - ETA: 36:24 - loss: 6.41 - ETA: 36:23 - loss: 6.40 - ETA: 36:22 - loss: 6.40 - ETA: 36:22 - loss: 6.40 - ETA: 36:21 - loss: 6.39 - ETA: 36:20 - loss: 6.39 - ETA: 36:20 - loss: 6.39 - ETA: 36:19 - loss: 6.39 - ETA: 36:18 - loss: 6.38 - ETA: 36:17 - loss: 6.39 - ETA: 36:16 - loss: 6.38 - ETA: 36:14 - loss: 6.39 - ETA: 36:13 - loss: 6.39 - ETA: 36:13 - loss: 6.39 - ETA: 36:12 - loss: 6.39 - ETA: 36:11 - loss: 6.39 - ETA: 36:09 - loss: 6.38 - ETA: 36:08 - loss: 6.38 - ETA: 36:08 - loss: 6.38 - ETA: 36:08 - loss: 6.38 - ETA: 36:08 - loss: 6.38 - ETA: 36:07 - loss: 6.38 - ETA: 36:08 - loss: 6.38 - ETA: 36:07 - loss: 6.38 - ETA: 36:07 - loss: 6.37 - ETA: 36:06 - loss: 6.37 - ETA: 36:06 - loss: 6.37 - ETA: 36:05 - loss: 6.36 - ETA: 36:06 - loss: 6.36 - ETA: 36:05 - loss: 6.36 - ETA: 36:04 - loss: 6.36 - ETA: 36:03 - loss: 6.36 - ETA: 36:02 - loss: 6.36 - ETA: 36:01 - loss: 6.36 - ETA: 36:01 - loss: 6.35 - ETA: 36:01 - loss: 6.35 - ETA: 36:00 - loss: 6.34 - ETA: 36:01 - loss: 6.34 - ETA: 35:59 - loss: 6.34 - ETA: 35:58 - loss: 6.34 - ETA: 35:57 - loss: 6.34 - ETA: 35:57 - loss: 6.34 - ETA: 35:55 - loss: 6.34 - ETA: 35:54 - loss: 6.33 - ETA: 35:54 - loss: 6.33 - ETA: 35:54 - loss: 6.33 - ETA: 35:54 - loss: 6.33 - ETA: 35:54 - loss: 6.33 - ETA: 35:54 - loss: 6.33 - ETA: 35:52 - loss: 6.32 - ETA: 35:51 - loss: 6.32 - ETA: 35:51 - loss: 6.32 - ETA: 35:50 - loss: 6.32 - ETA: 35:49 - loss: 6.32 - ETA: 35:49 - loss: 6.32 - ETA: 35:48 - loss: 6.32 - ETA: 35:46 - loss: 6.31 - ETA: 35:46 - loss: 6.32 - ETA: 35:45 - loss: 6.32 - ETA: 35:44 - loss: 6.32 - ETA: 35:43 - loss: 6.32 - ETA: 35:42 - loss: 6.32 - ETA: 35:41 - loss: 6.32 - ETA: 35:41 - loss: 6.33 - ETA: 35:40 - loss: 6.32 - ETA: 35:39 - loss: 6.32 - ETA: 35:39 - loss: 6.33 - ETA: 35:38 - loss: 6.33 - ETA: 35:37 - loss: 6.33 - ETA: 35:37 - loss: 6.33 - ETA: 35:36 - loss: 6.33 - ETA: 35:35 - loss: 6.33 - ETA: 35:35 - loss: 6.33 - ETA: 35:34 - loss: 6.33 - ETA: 35:33 - loss: 6.33 - ETA: 35:34 - loss: 6.33 - ETA: 35:33 - loss: 6.33 - ETA: 35:34 - loss: 6.33 - ETA: 35:33 - loss: 6.32 - ETA: 35:32 - loss: 6.32 - ETA: 35:32 - loss: 6.32 - ETA: 35:31 - loss: 6.33 - ETA: 35:30 - loss: 6.32 - ETA: 35:29 - loss: 6.32 - ETA: 35:28 - loss: 6.32 - ETA: 35:28 - loss: 6.32 - ETA: 35:27 - loss: 6.32 - ETA: 35:28 - loss: 6.32 - ETA: 35:26 - loss: 6.32 - ETA: 35:25 - loss: 6.32 - ETA: 35:24 - loss: 6.32 - ETA: 35:23 - loss: 6.31 - ETA: 35:22 - loss: 6.31 - ETA: 35:22 - loss: 6.31 - ETA: 35:21 - loss: 6.31 - ETA: 35:21 - loss: 6.31 - ETA: 35:20 - loss: 6.31 - ETA: 35:20 - loss: 6.31 - ETA: 35:19 - loss: 6.31 - ETA: 35:18 - loss: 6.31 - ETA: 35:17 - loss: 6.30 - ETA: 35:16 - loss: 6.30 - ETA: 35:16 - loss: 6.30 - ETA: 35:14 - loss: 6.30 - ETA: 35:14 - loss: 6.30 - ETA: 35:13 - loss: 6.30 - ETA: 35:13 - loss: 6.30 - ETA: 35:13 - loss: 6.30 - ETA: 35:12 - loss: 6.30 - ETA: 35:11 - loss: 6.29 - ETA: 35:10 - loss: 6.29 - ETA: 35:10 - loss: 6.29 - ETA: 35:09 - loss: 6.29 - ETA: 35:09 - loss: 6.29 - ETA: 35:08 - loss: 6.28 - ETA: 35:07 - loss: 6.28 - ETA: 35:06 - loss: 6.28 - ETA: 35:06 - loss: 6.28 - ETA: 35:05 - loss: 6.28 - ETA: 35:05 - loss: 6.27 - ETA: 35:04 - loss: 6.27 - ETA: 35:03 - loss: 6.27 - ETA: 35:02 - loss: 6.27 - ETA: 35:03 - loss: 6.27 - ETA: 35:02 - loss: 6.27 - ETA: 35:01 - loss: 6.27 - ETA: 35:00 - loss: 6.26 - ETA: 35:00 - loss: 6.26 - ETA: 34:58 - loss: 6.25 - ETA: 34:58 - loss: 6.25 - ETA: 34:58 - loss: 6.25 - ETA: 34:57 - loss: 6.25 - ETA: 34:57 - loss: 6.25 - ETA: 34:56 - loss: 6.25 - ETA: 34:56 - loss: 6.24 - ETA: 34:56 - loss: 6.24 - ETA: 34:55 - loss: 6.24 - ETA: 34:54 - loss: 6.24 - ETA: 34:54 - loss: 6.24 - ETA: 34:53 - loss: 6.24 - ETA: 34:53 - loss: 6.24 - ETA: 34:53 - loss: 6.24 - ETA: 34:52 - loss: 6.24 - ETA: 34:52 - loss: 6.25 - ETA: 34:50 - loss: 6.25 - ETA: 34:50 - loss: 6.25 - ETA: 34:49 - loss: 6.25 - ETA: 34:49 - loss: 6.25 - ETA: 34:48 - loss: 6.24 - ETA: 34:48 - loss: 6.24 - ETA: 34:48 - loss: 6.25 - ETA: 34:48 - loss: 6.25 - ETA: 34:48 - loss: 6.24 - ETA: 34:47 - loss: 6.25 - ETA: 34:46 - loss: 6.24 - ETA: 34:45 - loss: 6.24 - ETA: 34:44 - loss: 6.24 - ETA: 34:44 - loss: 6.24 - ETA: 34:43 - loss: 6.24 - ETA: 34:42 - loss: 6.24 - ETA: 34:41 - loss: 6.24 - ETA: 34:40 - loss: 6.24 - ETA: 34:40 - loss: 6.24 - ETA: 34:40 - loss: 6.23 - ETA: 34:40 - loss: 6.23 - ETA: 34:40 - loss: 6.23 - ETA: 34:39 - loss: 6.23 - ETA: 34:39 - loss: 6.23 - ETA: 34:38 - loss: 6.23 - ETA: 34:38 - loss: 6.22 - ETA: 34:37 - loss: 6.22 - ETA: 34:35 - loss: 6.22 - ETA: 34:36 - loss: 6.22 - ETA: 34:36 - loss: 6.22 - ETA: 34:35 - loss: 6.21 - ETA: 34:36 - loss: 6.21 - ETA: 34:35 - loss: 6.21 - ETA: 34:35 - loss: 6.21 - ETA: 34:35 - loss: 6.21 - ETA: 34:34 - loss: 6.21 - ETA: 34:33 - loss: 6.21 - ETA: 34:33 - loss: 6.21 - ETA: 34:31 - loss: 6.21 - ETA: 34:31 - loss: 6.21 - ETA: 34:31 - loss: 6.20 - ETA: 34:30 - loss: 6.20 - ETA: 34:31 - loss: 6.20 - ETA: 34:30 - loss: 6.20 - ETA: 34:30 - loss: 6.19 - ETA: 34:29 - loss: 6.19 - ETA: 34:29 - loss: 6.19 - ETA: 34:28 - loss: 6.19 - ETA: 34:27 - loss: 6.19 - ETA: 34:27 - loss: 6.18 - ETA: 34:27 - loss: 6.18 - ETA: 34:27 - loss: 6.18 - ETA: 34:26 - loss: 6.18 - ETA: 34:26 - loss: 6.17 - ETA: 34:25 - loss: 6.17 - ETA: 34:25 - loss: 6.17 - ETA: 34:24 - loss: 6.17 - ETA: 34:24 - loss: 6.17 - ETA: 34:23 - loss: 6.17 - ETA: 34:22 - loss: 6.16 - ETA: 34:22 - loss: 6.16 - ETA: 34:22 - loss: 6.16 - ETA: 34:22 - loss: 6.16 - ETA: 34:21 - loss: 6.15 - ETA: 34:20 - loss: 6.15 - ETA: 34:19 - loss: 6.15 - ETA: 34:19 - loss: 6.15 - ETA: 34:19 - loss: 6.15 - ETA: 34:18 - loss: 6.14 - ETA: 34:18 - loss: 6.14 - ETA: 34:17 - loss: 6.14 - ETA: 34:16 - loss: 6.14 - ETA: 34:16 - loss: 6.14 - ETA: 34:15 - loss: 6.14 - ETA: 34:14 - loss: 6.14 - ETA: 34:13 - loss: 6.13 - ETA: 34:13 - loss: 6.13 - ETA: 34:13 - loss: 6.13 - ETA: 34:11 - loss: 6.13 - ETA: 34:11 - loss: 6.13 - ETA: 34:10 - loss: 6.12 - ETA: 34:10 - loss: 6.12 - ETA: 34:09 - loss: 6.12 - ETA: 34:08 - loss: 6.12 - ETA: 34:07 - loss: 6.12 - ETA: 34:07 - loss: 6.12 - ETA: 34:05 - loss: 6.12 - ETA: 34:05 - loss: 6.12 - ETA: 34:04 - loss: 6.12 - ETA: 34:04 - loss: 6.11 - ETA: 34:05 - loss: 6.11 - ETA: 34:04 - loss: 6.11 - ETA: 34:03 - loss: 6.11 - ETA: 34:03 - loss: 6.11 - ETA: 34:02 - loss: 6.10 - ETA: 34:01 - loss: 6.10 - ETA: 34:01 - loss: 6.10 - ETA: 34:01 - loss: 6.09 - ETA: 34:00 - loss: 6.09 - ETA: 33:59 - loss: 6.09 - ETA: 33:59 - loss: 6.09 - ETA: 33:58 - loss: 6.08 - ETA: 33:57 - loss: 6.08 - ETA: 33:57 - loss: 6.08 - ETA: 33:56 - loss: 6.08 - ETA: 33:56 - loss: 6.08 - ETA: 33:56 - loss: 6.07 - ETA: 33:54 - loss: 6.07 - ETA: 33:54 - loss: 6.07 - ETA: 33:55 - loss: 6.07 - ETA: 33:54 - loss: 6.07 - ETA: 33:53 - loss: 6.06 - ETA: 33:53 - loss: 6.06 - ETA: 33:52 - loss: 6.06 - ETA: 33:51 - loss: 6.06 - ETA: 33:51 - loss: 6.06 - ETA: 33:50 - loss: 6.06 - ETA: 33:50 - loss: 6.06 - ETA: 33:49 - loss: 6.06 - ETA: 33:49 - loss: 6.06 - ETA: 33:49 - loss: 6.06 - ETA: 33:48 - loss: 6.06 - ETA: 33:48 - loss: 6.06 - ETA: 33:47 - loss: 6.06 - ETA: 33:47 - loss: 6.06 - ETA: 33:46 - loss: 6.06 - ETA: 33:45 - loss: 6.06 - ETA: 33:45 - loss: 6.06 - ETA: 33:44 - loss: 6.06 - ETA: 33:43 - loss: 6.06 - ETA: 33:43 - loss: 6.06 - ETA: 33:42 - loss: 6.05 - ETA: 33:43 - loss: 6.05 - ETA: 33:42 - loss: 6.05 - ETA: 33:41 - loss: 6.05 - ETA: 33:41 - loss: 6.05 - ETA: 33:40 - loss: 6.05 - ETA: 33:40 - loss: 6.05 - ETA: 33:39 - loss: 6.05 - ETA: 33:39 - loss: 6.05 - ETA: 33:38 - loss: 6.05 - ETA: 33:38 - loss: 6.05 - ETA: 33:38 - loss: 6.05 - ETA: 33:37 - loss: 6.05 - ETA: 33:37 - loss: 6.05 - ETA: 33:37 - loss: 6.05 - ETA: 33:37 - loss: 6.05 - ETA: 33:36 - loss: 6.05 - ETA: 33:35 - loss: 6.05 - ETA: 33:34 - loss: 6.05 - ETA: 33:34 - loss: 6.04 - ETA: 33:33 - loss: 6.05 - ETA: 33:33 - loss: 6.05 - ETA: 33:32 - loss: 6.04 - ETA: 33:32 - loss: 6.04"
     ]
    }
   ],
   "source": [
    "# Making a directory models to save our models\n",
    "\n",
    "os.mkdir(\"new_models\")\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n",
    "    model.save(\"new_models/model_\" + str(i) + \".h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
